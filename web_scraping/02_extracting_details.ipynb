{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c3481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "\n",
    "import time,random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import json\n",
    "import numpy as np\n",
    "import time, random, concurrent.futures\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_data = [] # loading the data\n",
    "\n",
    "with open('all_cars.json','r') as f:\n",
    "        data = json.load(f)\n",
    "        entire_data.extend(list(data.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2970147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver():\n",
    "    \"\"\"\n",
    "    This function do not expect any input, and it returns a webdriver object of chrome browser with few settings in options params\n",
    "    \"\"\"\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "\n",
    "    # It operates without opening a visible browser window.\n",
    "    options.add_argument(\"--headless=new\") \n",
    "\n",
    "    # Some servers (like Google Colab, Docker, or cloud machines) block Chrome from starting with its default safety settings. \n",
    "    # Adding this option makes sure Chrome doesnâ€™t crash and your scraper keeps running.\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "def scrape_page(link):\n",
    "    \"\"\"\n",
    "    This function take a link as input and extract each and every detail of car of that link a dict that holds result of perticular car\n",
    "    \"\"\"\n",
    "    driver = None\n",
    "\n",
    "    # --- init with np.nan to prevent data shifting issue ---\n",
    "    data = {\n",
    "        \"image_list\": np.nan,\n",
    "        \"new_used\": np.nan,\n",
    "        \"car_name\": np.nan,\n",
    "        \"mileage\": np.nan,\n",
    "        \"price\": np.nan,\n",
    "        \"price_drop\": np.nan,\n",
    "        \"deal_type\": np.nan,\n",
    "        \"key_specs\": np.nan,\n",
    "        \"basics\": np.nan,\n",
    "        \"features\": np.nan,\n",
    "        \"other_features\": np.nan,\n",
    "        \"vehicle_history\": np.nan,\n",
    "        \"seller_name\": np.nan,\n",
    "        \"seller_rating\": np.nan,\n",
    "        \"people_count_seller_rating\": np.nan,\n",
    "        \"seller_address\": np.nan,\n",
    "        \"seller_site\": np.nan,\n",
    "        \"car_rating\": np.nan,\n",
    "        \"people_count_car_rating\": np.nan,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "\n",
    "        # --- Creating a Driver Object ---\n",
    "        driver = get_driver()\n",
    "        driver.get(link)\n",
    "        time.sleep(random.choice([2, 3])) # time to get load\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\") \n",
    "\n",
    "        # --- Extract fields ---\n",
    "\n",
    "        # IMAGES\n",
    "        try:\n",
    "            imgs = []\n",
    "            selected_img = soup.find(\"img\", attrs={\"class\":\"swipe-main-image image-index-0\"})\n",
    "            if selected_img:\n",
    "                imgs.append(selected_img.get(\"src\"))\n",
    "            for image in soup.find_all(\"img\", attrs={\"class\":\"row-pic\"}):\n",
    "                imgs.append(image.get(\"src\"))\n",
    "            data[\"image_list\"] = imgs if imgs else np.nan\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # NEW OR USED\n",
    "        try:\n",
    "            data[\"new_used\"] = soup.find(\"p\",\"new-used\").text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # CAR NAME\n",
    "        try: \n",
    "            data[\"car_name\"] = soup.find(\"h1\",\"listing-title\").text\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # MILEAGE\n",
    "        try: \n",
    "            data[\"mileage\"] = soup.find(\"p\",\"listing-mileage\").text\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # PRICE\n",
    "        try: \n",
    "            data[\"price\"] = soup.find(\"span\", attrs={\"data-qa\":\"primary-price\"}).text\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # PRICE_DROP\n",
    "        try:\n",
    "            data[\"price_drop\"] = soup.find(\"span\", attrs={\"data-qa\":\"price-drop\"}).text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # DEAL TYPE\n",
    "        try: \n",
    "            data[\"deal_type\"] = soup.find(\"span\", attrs={\"data-qa\":\"price-badge-text\"}).text\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # --- Key Specs ---\n",
    "        keyspec_dic = {}\n",
    "        try:\n",
    "            for i in soup.find(\"div\",id=\"key-specs-container\").find(\"dl\",id=\"key-spec-details-container\").find_all(\"div\",\"key-spec\"):\n",
    "                keyspec_dic[i.find(\"dd\").text] = i.find(\"dt\").text\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            for i in soup.find(\"dl\",id=\"insight-details-container\").find_all(\"div\",\"key-spec\"):\n",
    "                keyspec_dic[i.find(\"dd\").text] = i.find(\"dt\").text\n",
    "        except:\n",
    "            pass\n",
    "        if keyspec_dic:\n",
    "            data[\"key_specs\"] = keyspec_dic\n",
    "\n",
    "        # --- Basics ---\n",
    "        try:\n",
    "            basics = {}\n",
    "            for dt,dd in zip(\n",
    "                soup.find(\"section\",\"sds-page-section basics-section\").find_all(\"dt\"),\n",
    "                soup.find(\"section\",\"sds-page-section basics-section\").find_all(\"dd\")\n",
    "            ):\n",
    "                basics[dt.text] = dd.text\n",
    "            data[\"basics\"] = basics\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # --- Features ---\n",
    "        try:\n",
    "            features_dic = {}\n",
    "            for dt,dd in zip(\n",
    "                soup.find(\"section\",\"sds-page-section features-section\").find_all(\"dt\"),\n",
    "                soup.find(\"section\",\"sds-page-section features-section\").find(\"dl\",\"fancy-description-list\").find_all(\"dd\")\n",
    "            ):\n",
    "                features_dic[dt.text] = dd.text\n",
    "            data[\"features\"] = features_dic\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # Other Features\n",
    "        try: \n",
    "            data[\"other_features\"] = [li.text for li in soup.find(\"div\",\"sds-modal sds-modal-one view-all-features-container\").find_all(\"li\")]\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # Vehicle History\n",
    "        try:\n",
    "            history_dic = {}\n",
    "            for dt,dd in zip(\n",
    "                soup.find(\"section\",\"sds-page-section vehicle-history-section\").find_all(\"dt\"),\n",
    "                soup.find(\"section\",\"sds-page-section vehicle-history-section\").find_all(\"dd\")\n",
    "            ):\n",
    "                history_dic[dt.text] = dd.text\n",
    "            data[\"vehicle_history\"] = history_dic\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # Seller Name\n",
    "        try: \n",
    "            data[\"seller_name\"] = soup.find(\"h3\",\"spark-heading-5 heading seller-name\").text\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # Seller Rating\n",
    "        try: \n",
    "            data[\"seller_rating\"] = soup.find(\"spark-rating\").get(\"rating\")\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # People count that rated seller\n",
    "        try: \n",
    "            data[\"people_count_seller_rating\"] = soup.find(\"spark-rating\").text\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # Seller Address\n",
    "        try: \n",
    "            data[\"seller_address\"] = soup.find(\"div\",\"dealer-address\").text\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # Seller Site\n",
    "        try: \n",
    "            data[\"seller_site\"] = soup.find(\"a\",{\"data-linkname\":\"dealer-external-site\"}).get(\"href\")\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # Car rating\n",
    "        temp = None\n",
    "        try:\n",
    "            temp = soup.find(\"div\",\"vehicle-reviews generated-consumer-review-shown\").find(\"spark-rating\",attrs={\"size\":\"overview\"}).get(\"rating\")\n",
    "        except: pass\n",
    "        if not temp:\n",
    "            try: temp = soup.find(\"div\",\"vehicle-reviews\").find(\"spark-rating\",attrs={\"size\":\"overview\"}).get(\"rating\")\n",
    "            except: pass\n",
    "        data[\"car_rating\"] = temp if temp else np.nan\n",
    "\n",
    "        # People count car rating\n",
    "        temp = None\n",
    "        try:\n",
    "            temp = soup.find(\"div\",\"vehicle-reviews generated-consumer-review-shown\").find(\"spark-rating\",attrs={\"size\":\"overview\"}).find(\"a\").text\n",
    "        except: pass\n",
    "        if not temp:\n",
    "            try: temp = soup.find(\"div\",\"vehicle-reviews\").find(\"spark-rating\",attrs={\"size\":\"overview\"}).find(\"a\").text\n",
    "            except: pass\n",
    "        data[\"people_count_car_rating\"] = temp if temp else np.nan\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {link}: {e}\")\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "    return data\n",
    "\n",
    "results = [] # this list will hold all dicts\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor: # At most 4 tasks will run at the same time (in parallel, using threads).\n",
    "    for idx, result in enumerate(executor.map(scrape_page, entire_data[322000:323000])):\n",
    "        results.append(result)\n",
    "        print(f\"{idx+1}/{len(entire_data)} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.to_csv('all-cars.csv') # exporting the results in csv file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160fb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, instead of doing this entire work alone on my single local machine, I used colabs from my 3 different emails to prevent getting \n",
    "# blocked by server and executing the work faster, but because there are too many links, slow response from server I also used the kaggle notebook\n",
    "# hence instead of using just 1 local machine, I used (5 (kaggle_server) + 3 colab server) * 3 mails = 32 server in total\n",
    "\n",
    "# I scraped the data and stored each file in final_uncleaned_data folder , which will be further used for cleaning, but to reduce the size\n",
    "# I converted the data into parquet format (stores data into binary format, and divided entire data into 5 diff chunks in total)\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "folder_path = \"C:/Users/aksha/OneDrive/Desktop/CARS-FINAL_YEAR_PROJECT/final_uncleaned_data\"\n",
    "\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "df = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df = df.sample(335956) # shuffled the data\n",
    "\n",
    "n_splits = 5 \n",
    "rows_per_file = len(df) // n_splits  \n",
    "\n",
    "for i in range(n_splits):\n",
    "    start = i * rows_per_file\n",
    "    end = (i + 1) * rows_per_file if i < n_splits - 1 else len(df)\n",
    "\n",
    "    df_chunk = df.iloc[start:end] \n",
    "\n",
    "    print(df_chunk.shape)\n",
    "\n",
    "    df_chunk.to_parquet(f\"data_part_{i+1}.parquet\", index=False) # exporting to chunks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
